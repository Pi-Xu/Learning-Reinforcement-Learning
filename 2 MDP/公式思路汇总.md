# 公式思路

本章的重要内容是一些公式的推导, 即如何从Value Function以及Q Function的角度给出MDP(或MRP中的控制问题).

## MDP 与 MRP 区别

- MRP
  - 马尔可夫奖励过程(Markov Reward Process, MRP) 是马尔可夫链再加上了一个奖励函数。在 MRP 中，转移矩阵和状态都是跟马尔可夫链一样的，多了一个奖励函数(reward function)。奖励函数 $R$ 是一个期望，就是说当你到达某一个状态的时候，可以获得多大的奖励，然后这里另外定义了一个 discount factor $\gamma$ 。如果状态数是有限的，$R$ 可以是一个向量。
- MDP
  - 相对于 MRP，马尔可夫决策过程(Markov Decision Process)多了一个 decision，其它的定义跟 MRP 都是类似的:
- 在第一章中对Value Function进行了定义
  - $v^{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s\right]$

### 推导 Value Function (MRP中)

$$
V(s)=\underbrace{R(s)}_{\text {Immediate reward }}+\underbrace{\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {Discounted sum of future reward }}\tag{1}
$$

该式子反映了当前状态的价值和未来状态价值的关系. 关键的推导主要使用[全期望公式](https://datawhalechina.github.io/easy-rl/#/chapter2/chapter2?id=law-of-total-expectation)的方法, 其主要需要使用下式(2)在写开之后的某一步中进行迭代.

$$
\mathbb{E}\left[V\left(s_{t+1}\right) \mid s_{t}\right]=\mathbb{E}\left[\mathbb{E}\left[G_{t+1} \mid s_{t+1}\right] \mid s_{t}\right]=E\left[G_{t+1} \mid s_{t}\right] \tag{2}
$$

将上式写作一个更简洁的表达是:

$$
\left[\begin{array}{c}
V\left(s_{1}\right) \\
V\left(s_{2}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right]=\left[\begin{array}{c}
R\left(s_{1}\right) \\
R\left(s_{2}\right) \\
\vdots \\
R\left(s_{N}\right)
\end{array}\right]+\gamma\left[\begin{array}{cccc}
P\left(s_{1} \mid s_{1}\right) & P\left(s_{2} \mid s_{1}\right) & \ldots & P\left(s_{N} \mid s_{1}\right) \\
P\left(s_{1} \mid s_{2}\right) & P\left(s_{2} \mid s_{2}\right) & \ldots & P\left(s_{N} \mid s_{2}\right) \\
\vdots & \vdots & \ddots & \vdots \\
P\left(s_{1} \mid s_{N}\right) & P\left(s_{2} \mid s_{N}\right) & \ldots & P\left(s_{N} \mid s_{N}\right)
\end{array}\right]\left[\begin{array}{c}
V\left(s_{1}\right) \\
V\left(s_{2}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right]
$$

## 从Bellman Equation开始的公式推导

### Q function的Bellman equation

首先声明其定义:

$$
q(s, a) = \mathbb{E} [G_t \mid s_t, a_t] \\
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a) \tag{3}
$$

在此基础上, 计算其Bellman Equation, 中间同样会使用到一些技巧, 参见Easy RL中的介绍即可.

$$
q(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right) \tag{4}
$$

### 从 Bellman Equation到现在与未来之间的关系

从(3, 4)等公式就可以很好地推导出相关的公式, 现在与未来之间的关系:

$$
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)\right) \tag{5}
$$

注意对比公式(1)与公式(5)之间的差异. (5)在(1)的基础之上增加了与Policy有关的部分.

$$
q^{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q^{\pi}\left(s^{\prime}, a^{\prime}\right)
$$
