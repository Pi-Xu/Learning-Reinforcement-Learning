# 公式思路汇总

上一章中的定义:

$$
\triangledown \bar R_\theta = \mathbb{E}_{\tau \sim p_\theta (\tau)} [R(\tau) \triangledown \log p_\theta(\tau) ]
$$

## 重要性采样

motivation: 每次都要重新根据$\theta$的更新进行采样, 很麻烦.

定义:

$$
\mathbb{E}_{x\sim p} [f(x)] = \mathbb{E}_{x \sim q} [f(x) \frac{p(x)}{q(x)}]
$$

本章的应用:

$$
\triangledown \bar R_\theta = \mathbb{E}_{\tau \sim p_{\theta ^{'}}(\tau)} [\frac{p_\theta(\tau)}{p_{\theta^{'}} (\tau)} R(\tau) \triangledown \log p_\theta(\tau)]
$$

只要采样数量足够多, 就能很好逼近. 同时也可以减少重新采用数量, 梯度下降几轮再更新一次$\theta^{'}$即可, 不用像之前一样每次都更新.

## PPO 算法

motivation: 两个分布相差太多，重要性采样的结果就会不好, 基于上述内容引出近端优化

近端优化1:

$$
J ^{\theta^k}_{PPO}(\theta) = J^{\theta^k}(\theta ) - \beta \text{KL}(\theta, \theta^k)\\
\text{其中}J^{\theta^k}(\theta) = \mathbb{E}_{(s_t, a_t) \sim \pi_{\theta^k} }[\frac{p_\theta(a_t \mid s_t)}{p_{\theta^k}(a_t \mid s_t)} A^{\theta^k}(s_t, a_t)], \\
p_\theta(s_t) = p_{\theta^k}(s_t)
$$

近端优化2:

$$
\begin{aligned}
J_{\mathrm{PPO} 2}^{\theta^{k}}(\theta) \approx \sum_{\left(s_{t}, a_{t}\right)} &\min \left(\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right),\right.\\
&\left.\operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right)
\end{aligned}
$$
